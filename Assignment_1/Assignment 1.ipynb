{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2024 Semester 1\n",
    "\n",
    "## Assignment 1: Wine quality classification with K-NN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student ID(s):**     `1356034`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you will use for your Assignment 1 submission.\n",
    "\n",
    "Marking will be applied on the four functions that are defined in this notebook, and to your responses to the questions at the end of this notebook (Submitted in a separate PDF file).\n",
    "\n",
    "**NOTE: YOU SHOULD ADD YOUR RESULTS, DIAGRAMS AND IMAGES FROM YOUR OBSERVATIONS IN THIS FILE TO YOUR REPORT (the PDF file).**\n",
    "\n",
    "You may change the prototypes of these functions, and you may write other functions, according to your requirements. We would appreciate it if the required functions were prominent/easy to find.\n",
    "\n",
    "**Adding proper comments to your code is MANDATORY. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE SURE TO RUN FIRST: IMPORT ALL LIBRARIES REQUIRED FOR ALL FUNCTIONS IN THIS NOTEBOOK\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "train_orig = \"winequality-train.csv\"\n",
    "test_orig = \"winequality-test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. K-NN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS FUNCTION RUNS K-NN CLASSIFICATION - CALL classifier TO CLASSIFY A GIVEN TRAIN-TEST DATASET\n",
    "# Takes a training filepath, a test filepath, a value for k as well as a filepath for classification results output\n",
    "# Outputs its predictions in the \"predictedQuality\" column in the output file\n",
    "def classifier(train_path: str, test_path: str, k: int, out_file: str):\n",
    "    # read each csv file into a pandas dataframe\n",
    "    train = pd.read_csv(train_path)\n",
    "    test = pd.read_csv(test_path)\n",
    "    \n",
    "    # prohibit k values which are outside of the specified bounds\n",
    "    if (k < 1 or k > train.shape[0]):\n",
    "        raise Exception(f\"k must be between 1 and {train.shape[0]} inclusive\")\n",
    "\n",
    "    test['predictedQuality'] = test.apply(lambda row: predict_label(row, train, k), axis = 1)\n",
    "    test.to_csv(out_file, index = False)\n",
    "    print(f\"Classification finished - check output file {out_file} for results\")\n",
    "    \n",
    "\n",
    "def predict_label(test_row, train, k):\n",
    "    # calculate the euclidean distance between the current test row and all of the training rows\n",
    "    train = train.drop(['euclideanDistance'], axis = 1, errors = 'ignore')\n",
    "    train['euclideanDistance'] = train.apply(lambda train_row: calculate_distance(train_row, test_row), axis = 1)\n",
    "    \n",
    "    return break_ties(train, k)\n",
    "    \n",
    "    \n",
    "def break_ties(df, k):\n",
    "    # find the k smallest distances including tied values\n",
    "    k_items = df.nsmallest(k, 'euclideanDistance', keep = 'all')\n",
    "    # get the counts for how many times each label appears\n",
    "    counts_series = k_items['quality'].value_counts()\n",
    "    # create a set of these counts for easy comparison later\n",
    "    value_set = set(counts_series)\n",
    "        \n",
    "    # if we have a tie between counts for the two labels\n",
    "    if (len(value_set) == 1 and max(value_set) != k_items.shape[0]):\n",
    "        \n",
    "        if (k_items.shape[0] != k or k == 1):\n",
    "            # tie due to two points being at the same distance\n",
    "            # choose a random label and return\n",
    "            return random.choose(counts_series.index)\n",
    "        else:\n",
    "            # tie not due to two points being at the same distance\n",
    "            # re-run the function again with 1-nn\n",
    "            return break_ties(k_items, 1)\n",
    "        \n",
    "    else:\n",
    "        # no ties are occurring\n",
    "        # return the value of the label that appears most frequently\n",
    "        return counts_series.idxmax()\n",
    "    \n",
    "\n",
    "def calculate_distance(train, test):\n",
    "    distance = 0\n",
    "    \n",
    "    # find the euclidean distance between all of the features (excluding quality)\n",
    "    for i in range(0, len(test) - 1):\n",
    "        distance += ((train.iloc[i] - test.iloc[i]) ** 2)\n",
    "\n",
    "    # no need to square root the distance as we are only worried about the relative magnitude to other distances\n",
    "    # saves computation time\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification finished - check output file output_task1.csv for results\n"
     ]
    }
   ],
   "source": [
    "# THE FUNCTION CALL BELOW RUNS THE ABOVE CLASSIFICATION FUNCTION\n",
    "# RUNS ON TRAIN_ORIG AND TEST_ORIG WITH K=1 AND DIRECT OUTPUT TO task1_output.csv\n",
    "\n",
    "classifier(train_orig, test_orig, 1, \"output_task1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 1-NN classification\n",
    "\n",
    "#### NOTE: you may develop codes or functions to help respond to the question here, but your formal answer must be submitted separately as a PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS FUNCTION CALCULATES ACCURACY OF A CLASSIFIED TEST FILE\n",
    "# Takes a results filepath with columns 'quality' and 'predictedQuality' as input\n",
    "def calculate_accuracy(in_file: str):\n",
    "    # calculate the accuracy of the test file using SKLearn\n",
    "    results = pd.read_csv(in_file)\n",
    "    actual = results['quality'].values\n",
    "    \n",
    "    predicted = results['predictedQuality'].values\n",
    "    return accuracy_score(actual, predicted)\n",
    "\n",
    "# THIS FUNCTION PRINTS THE CLASS DISTRIBUTION IN A GIVEN DATA FILE\n",
    "# Takes a filepath as input\n",
    "def calculate_class_distribution(in_file: str):\n",
    "    # return the number of instances which are in each feature class\n",
    "    data = pd.read_csv(in_file)\n",
    "    counts_series = data['quality'].value_counts()\n",
    "    return(counts_series)\n",
    "\n",
    "# THIS FUNCTION PLOTS SCATTER PLOTS FOR TWO GIVEN ATTRIBUTES AND A GIVEN TRAINING FILE\n",
    "# ALSO HAS THE OPTION TO PLOT A RANDOM SUBSET AND TO ADD A TRENDLINE\n",
    "# Takes a training file, two attribute names and boolean values to add a trendline or to plot a subset of the data as input\n",
    "def plot_graphs(data_file: str, att1: str, att2: str, trendline: bool, subset: bool):\n",
    "    df = pd.read_csv(data_file)\n",
    "\n",
    "    # create a scatter plot between the two chosen attributes\n",
    "    # colour the plots according to their quality label\n",
    "    if subset:\n",
    "        # plot a subset of the dataframe - randomly selects and plots 1/4 of the data\n",
    "        sample_df = df.sample(len(df)//4)\n",
    "        sns.scatterplot(x = sample_df[att1], y = sample_df[att2], alpha=0.4, hue = df['quality'])\n",
    "    else:\n",
    "        sns.scatterplot(x = df[att1], y = df[att2], alpha=0.4, hue = df['quality'])\n",
    "        \n",
    "    plt.xlabel(att1)\n",
    "    plt.ylabel(att2)\n",
    "    plt.title(f'{att1} vs {att2} {data_file}')\n",
    "\n",
    "    # add a trendline to the data to show dependence between two attributes\n",
    "    if trendline:\n",
    "        z = np.polyfit(df[att1], df[att2], 1)\n",
    "        p = np.poly1d(z)\n",
    "        plt.plot(df[att1], p(df[att1]), \"r-\")\n",
    "    \n",
    "    plt.savefig(f'Scatter {att1} vs {att2} {data_file}.png')\n",
    "    plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of 0.7644444444444445 without scaling\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# THE BELOW FUNCTION CALLS RUN 1-NN ON THE ORIGINAL DATA AND THEN CALCULATE ACCURACY\n",
    "results_1nn = \"results_1nn.csv\"\n",
    "classifier(train_orig, test_orig, 1, results_1nn)\n",
    "accuracy = calculate_accuracy(results_1nn)\n",
    "print(f\"Accuracy score of {accuracy} without scaling\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distributions for TRAIN are: quality\n",
      "0    820\n",
      "1    530\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# THE BELOW FUNCTIONS PRINT THE CLASS DISTRIBUTION IN THE TRAINING FILE AS WELL AS CREATE SCATTER PLOTS FOR TWO GIVEN ATTRIBUTES\n",
    "class_distribution = calculate_class_distribution(train_orig)\n",
    "print(f\"Class distributions for TRAIN are: {class_distribution}\\n\")\n",
    "\n",
    "plot_graphs(train_orig, 'totalSulfurDioxide', 'citricAcid', False, True)\n",
    "plot_graphs(train_orig, 'chlorides', 'alcohol', False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Normalization\n",
    "\n",
    "#### NOTE: you may develop codes or functions to help respond to the question here, but your formal answer must be submitted separately as a PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS FUNCTION NORMALISES A TRAINING AND TEST FILE ACCORDING TO THE SPECIFIED NORMALISATION METHOD\n",
    "# Takes a training and test file, an output training and test path, as well as a boolean (true: minmax, false: std) as input\n",
    "def normalise_values(train_path: str, test_path: str, train_out: str, test_out: str, minmax: bool):\n",
    "    train = pd.read_csv(train_path)\n",
    "    test = pd.read_csv(test_path)\n",
    "    \n",
    "    if minmax:\n",
    "        # scale all columns except for quality\n",
    "        for column in train.columns[:-1]:\n",
    "            # find the min and max value for each attribute using the training data\n",
    "            min = train[column].min()\n",
    "            max = train[column].max()\n",
    "\n",
    "            # use the min max scale formula to scale all columns in both training and test datasets\n",
    "            train[column] = train.apply(lambda curr_row: calculate_minmax(min, max, curr_row[column]), axis = 1)\n",
    "            test[column] = test.apply(lambda curr_row: calculate_minmax(min, max, curr_row[column]), axis = 1)\n",
    "            \n",
    "    else:\n",
    "        for column in train.columns[:-1]:\n",
    "            # find the mean and std dev for each attribute using the training data\n",
    "            mean = train[column].mean()\n",
    "            stddev = train[column].std()\n",
    "\n",
    "            # use the standardisation formula to scale all columns in both training and test datasets\n",
    "            train[column] = train.apply(lambda curr_row: calculate_std(mean, stddev, curr_row[column]), axis = 1)\n",
    "            test[column] = test.apply(lambda curr_row: calculate_std(mean, stddev, curr_row[column]), axis = 1)\n",
    "            \n",
    "    train.to_csv(train_out, index = False)\n",
    "    test.to_csv(test_out, index = False)\n",
    "\n",
    "\n",
    "# the two functions below implement the min max and standardisation formulas respectively when called\n",
    "def calculate_minmax(min, max, val):\n",
    "    return (val-min)/(max-min)\n",
    "\n",
    "\n",
    "def calculate_std(mean, stddev, val):\n",
    "    return (val-mean)/stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification finished - check output file results_minmax.csv for results\n",
      "Accuracy score of 0.8503703703703703 for min max scaling\n",
      "\n",
      "Classification finished - check output file results_std.csv for results\n",
      "Accuracy score of 0.8674074074074074 for standardisation scaling\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# output file names for below functions\n",
    "train_minmax = \"train_minmax.csv\"\n",
    "test_minmax = \"test_minmax.csv\"\n",
    "train_std = \"train_std.csv\"\n",
    "test_std = \"test_std.csv\"\n",
    "results_minmax = \"results_minmax.csv\"\n",
    "results_std = \"results_std.csv\"\n",
    "\n",
    "# normalise the dataset using both methods\n",
    "normalise_values(train_orig, test_orig, train_minmax, test_minmax, True)\n",
    "normalise_values(train_orig, test_orig, train_std, test_std, False)\n",
    "\n",
    "# classify the minmax data and find accuracy\n",
    "classifier(train_minmax, test_minmax, 1, results_minmax)\n",
    "minmax_accuracy = calculate_accuracy(results_minmax)\n",
    "print(f\"Accuracy score of {minmax_accuracy} for min max scaling\\n\")\n",
    "\n",
    "# classify the standardised data and find accuracy\n",
    "classifier(train_std, test_std, 1, results_std)\n",
    "std_accuracy = calculate_accuracy(results_std)\n",
    "print(f\"Accuracy score of {std_accuracy} for standardisation scaling\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot graphs for totalSulfurDioxide vs freeSulfurDioxide for the three different train files\n",
    "plot_graphs(train_orig, 'totalSulfurDioxide', 'freeSulfurDioxide', False, False)\n",
    "plot_graphs(train_minmax, 'totalSulfurDioxide', 'freeSulfurDioxide', False, False)\n",
    "plot_graphs(train_std, 'totalSulfurDioxide', 'freeSulfurDioxide', False, False)\n",
    "\n",
    "# plot graphs for citricAcid vs pH for the three different train files\n",
    "plot_graphs(train_orig, 'citricAcid', 'pH', False, False)\n",
    "plot_graphs(train_minmax, 'citricAcid', 'pH', False, False)\n",
    "plot_graphs(train_std, 'citricAcid', 'pH', False, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model extensions\n",
    "\n",
    "#### NOTE: you may develop codes or functions to help respond to the question here, but your formal answer must be submitted separately as a PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1\n",
    "Compare the performance of your best 1-NN model from Question 3 to a Gaussian naive Bayes model on this dataset (you may use library functions to implement the Gaussian naive Bayes model). In your write-up, state the accuracy of the naive Bayes model and identify instances where the two models disagree. Why do the two models classify these instances differently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS FUNCTION CREATES A GNB CLASSIFIER FOR A GIVEN TRAINING DATASET AND CALCULATES ITS ACCURACY TO A TEST DATASET\n",
    "# Takes a training and test dataset file as well as a results output filepath as input\n",
    "def gnb_classifier(train_in, test_in, file_out):\n",
    "    train = pd.read_csv(train_in)\n",
    "    test = pd.read_csv(test_in)\n",
    "\n",
    "    # create a GNB classifier and fit it on the training data\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(train.iloc[:,:-1], train.iloc[:,-1])\n",
    "\n",
    "    # test the model's accuracy by predicting results on the test data\n",
    "    accuracy = gnb.score(test.iloc[:,:-1], test.iloc[:,-1])\n",
    "    print(f\"Accuracy score of {accuracy} for GNB model\")\n",
    "\n",
    "    # output the posterior probabilities that GNB computed for each instance\n",
    "    prob_table = gnb.predict_proba(test.iloc[:,:-1])\n",
    "    test['predictedQuality'] = gnb.predict(test.iloc[:,:-1])\n",
    "    test['0prob'] = prob_table[:,0]\n",
    "    test['1prob'] = prob_table[:,1] \n",
    "    \n",
    "    test.to_csv(file_out, index = False)\n",
    "\n",
    "# THIS FUNCTION PLOTS HISTOGRAMS FOR A GIVEN ATTRIBUTE IN A TRAINING DATASET SEPARATED BY CLASS LABEL\n",
    "# Takes a training filepath and the name of an attribute to make histograms for\n",
    "def make_histograms(train_in, att):\n",
    "    # create two separate datatables for instances with 0 or 1 quality\n",
    "    train = pd.read_csv(train_in)\n",
    "    train_0 = train[train['quality'] == 0]\n",
    "    train_1 = train[train['quality'] == 1]\n",
    "\n",
    "    # create a histogram for the instances with 0 quality\n",
    "    train_0.hist(att, bins = 20)\n",
    "    plt.xlabel(f'{att} Value')\n",
    "    plt.ylabel('Number of Occurrences')\n",
    "    plt.title(f'Histogram for {att} Label 0')\n",
    "    plt.savefig(f'{att} Label0 Histogram.png')\n",
    "    plt.close(\"all\")\n",
    "\n",
    "    # create a histogram for the instances with 1 quality\n",
    "    train_1.hist(att, bins = 20)\n",
    "    plt.xlabel(f'{att} Value')\n",
    "    plt.ylabel('Number of Occurrences')\n",
    "    plt.title(f'Histogram for {att} Label 1')\n",
    "    plt.savefig(f'{att} Label1 Histogram.png')\n",
    "    plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of 0.774074074074074 for GNB model\n"
     ]
    }
   ],
   "source": [
    "# run GNB classification on the standardised data\n",
    "gnb_file = \"results_gnb.csv\"\n",
    "gnb_classifier(train_std, test_std, gnb_file)\n",
    "\n",
    "# plot histograms and graphs to demonstrate reasons for lower performance\n",
    "plot_graphs(train_std, 'totalSulfurDioxide', 'freeSulfurDioxide', True, False)\n",
    "make_histograms(train_std, 'pH')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2\n",
    "Implement two additional distance measures for your K-NN model: cosine similarity and Mahalanobis distance (you may use library functions for these distance measures). Do 1-NN classification using each of these new distance measures and the three normalization options from Question 3. Discuss how the new distance metrics compare to Euclidean distance and how each metric is affected by normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3\n",
    "Implement either of the two K-NN weighting strategies discussed in lecture (inverse linear distance or inverse distance). Compare the performance of the weighted and majority vote models for a few different values of K. In your write-up, discuss how weighting strategy and the value of K affect the model's decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4\n",
    "Measure the empirical distribution of class labels in the training dataset (what percentage of the training data comes from each class). Then evaluate the distribution of labels predicted by your K-NN model for the test data, for a range of values for K. Does the class distribution of the predicted labels match the class distribution of the training data? Explain why or why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
