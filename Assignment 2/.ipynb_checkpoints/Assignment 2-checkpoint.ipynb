{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ab3799b",
   "metadata": {},
   "source": [
    "# Machine Learning Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e785b4a",
   "metadata": {},
   "source": [
    "### By Alexander de Vryer (1356034)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eb5940",
   "metadata": {},
   "source": [
    "## Necessary Imports and Helper Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ef89623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from statistics import mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eade98ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the csv files into a df\n",
    "train = pd.read_csv('train_dataset.csv')\n",
    "test = pd.read_csv('test_dataset.csv')\n",
    "\n",
    "# test prediction format used for final model output\n",
    "test_predictions = pd.DataFrame(test['id'])\n",
    "\n",
    "\n",
    "# helper functions used throughout\n",
    "def one_hot_encoder(train, test, att):\n",
    "    ohc = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "    train_element = pd.DataFrame(ohc.fit_transform(train[att].to_numpy().reshape(-1, 1)))\n",
    "    test_element = pd.DataFrame(ohc.transform(test[att].to_numpy().reshape(-1, 1)))\n",
    "    return train_element, test_element\n",
    "\n",
    "# draw a scatter plot of the data\n",
    "def draw_plot(att1):\n",
    "    sns.scatterplot(x = train[att1], y = train['imdb_score_binned'], alpha=0.4)\n",
    "    plt.xlabel(att1)\n",
    "    plt.ylabel('imdb_score_binned')\n",
    "    plt.title(f'{att1} vs imdb_score')\n",
    "    plt.savefig(f'Scatter {att1} vs imdb_score.png')\n",
    "    plt.close(\"all\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52792fd",
   "metadata": {},
   "source": [
    "## Read Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cd745e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the np arrays\n",
    "train_actor_1_name = pd.DataFrame(np.load('train_countvec_features_actor_1_name.npy'))\n",
    "test_actor_1_name = pd.DataFrame(np.load('test_countvec_features_actor_1_name.npy'))\n",
    "\n",
    "train_actor_2_name = pd.DataFrame(np.load('train_countvec_features_actor_2_name.npy'))\n",
    "test_actor_2_name = pd.DataFrame(np.load('test_countvec_features_actor_2_name.npy'))\n",
    "\n",
    "train_director_name = pd.DataFrame(np.load('train_countvec_features_director_name.npy'))\n",
    "test_director_name = pd.DataFrame(np.load('test_countvec_features_director_name.npy'))\n",
    "\n",
    "train_genres = pd.DataFrame(np.load('train_doc2vec_features_genre.npy'))\n",
    "test_genres = pd.DataFrame(np.load('test_doc2vec_features_genre.npy'))\n",
    "\n",
    "train_plot_keywords = pd.DataFrame(np.load('train_doc2vec_features_plot_keywords.npy'))\n",
    "test_plot_keywords = pd.DataFrame(np.load('test_doc2vec_features_plot_keywords.npy'))\n",
    "\n",
    "train_title_embedding = pd.DataFrame(np.load('train_fasttext_title_embeddings.npy'))\n",
    "test_title_embedding = pd.DataFrame(np.load('test_fasttext_title_embeddings.npy'))\n",
    "\n",
    "# apply one-hot encoding to categorical features\n",
    "train_content_rating, test_content_rating = one_hot_encoder(train, test, 'content_rating')\n",
    "train_language, test_language = one_hot_encoder(train, test, 'language')\n",
    "train_country, test_country = one_hot_encoder(train, test, 'country')\n",
    "\n",
    "\n",
    "# drop all non-numeric features from the main df\n",
    "features_to_drop = ['id', 'actor_1_name', 'actor_2_name', 'actor_3_name', 'director_name', 'movie_title', \n",
    "                    'genres', 'language', 'country', 'content_rating', 'title_year', 'plot_keywords', \n",
    "                    'title_embedding']\n",
    "\n",
    "for feature in features_to_drop:\n",
    "    train = train.drop(feature, axis='columns')\n",
    "    test = test.drop(feature, axis='columns')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a329764",
   "metadata": {},
   "source": [
    "# Feature Engineering:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcc9629",
   "metadata": {},
   "source": [
    "## Normalisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "004dbe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the numerical attributes we want to standardise\n",
    "numerical_attributes = ['num_critic_for_reviews', 'duration', 'director_facebook_likes', 'actor_3_facebook_likes', \n",
    "                        'actor_1_facebook_likes', 'gross', 'num_voted_users', 'cast_total_facebook_likes', \n",
    "                        'facenumber_in_poster', 'num_user_for_reviews', 'actor_2_facebook_likes', \n",
    "                        'movie_facebook_likes', 'average_degree_centrality']\n",
    "\n",
    "# standardise all numeric attributes to have mean 0 and std dev 1\n",
    "std = StandardScaler()\n",
    "train[numerical_attributes] = std.fit_transform(train[numerical_attributes])\n",
    "test[numerical_attributes] = std.transform(test[numerical_attributes])\n",
    "\n",
    "train_genres = std.fit_transform(train_genres)\n",
    "test_genres = std.transform(test_genres)\n",
    "\n",
    "train_plot_keywords = std.fit_transform(train_plot_keywords)\n",
    "test_plot_keywords = std.transform(test_plot_keywords)\n",
    "\n",
    "train_title_embedding = std.fit_transform(train_title_embedding)\n",
    "test_title_embedding = std.transform(test_title_embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6536c621",
   "metadata": {},
   "source": [
    "## Feature Selection using Mutual Information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55bd0d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original continuous features: [0.17680579485444792, 0.09349919324041922, 0.0829956328122119, 0.08072565833912648, 0.0650280251135027, 0.0633598594307263, 0.038647284262994575, 0.0377363311929404, 0.03723657609921416, 0.03482002316325028]\n",
      "\n",
      "Actor 1: [0.003534217741481222, 0.0034572371302196755, 0.0033873906249415435, 0.0033873906249415435, 0.002951848946812396, 0.002639922618862164, 0.002550624480618293, 0.0023502125095054035, 0.0022063069268572657, 0.002179560531694472]\n",
      "\n",
      "Actor 2: [0.002934984662683963, 0.002934984662683963, 0.0028764549617678447, 0.002403405733233436, 0.0023317962851738617, 0.0023150125224301404, 0.0023137696470028495, 0.0021510663378194227, 0.002100763620702814, 0.002100763620702814]\n",
      "\n",
      "Director: [0.004820623907788771, 0.004503269364648477, 0.0043202268406079165, 0.003520564833607431, 0.0034568063407268445, 0.0034568063407268445, 0.003154900682523741, 0.003154900682523741, 0.002625567762462727, 0.0024742706677133713]\n",
      "\n",
      "Genres: [0.049570624646464, 0.04418245142969557, 0.0435725530754163, 0.04309246979246861, 0.042904312852572346, 0.04210716601997522, 0.04150969482103539, 0.04148351147544016, 0.04131036168304769, 0.04082244150100811]\n",
      "\n",
      "Plot keywords: [0.01684460308556268, 0.014271222223295243, 0.012696221118968953, 0.012429510389657228, 0.012359619554692891, 0.0114518593425319, 0.011392054601041757, 0.010971115641499996, 0.010206820921643267, 0.009201579658304038]\n",
      "\n",
      "Title embeddings: [0.01764136888144696, 0.016708629335639102, 0.01599405247546093, 0.015686259764036148, 0.015189872899352697, 0.014874648740756502, 0.014459904888726616, 0.012544982207944422, 0.012312335036907651, 0.012111473769266023]\n",
      "\n",
      "Content rating: [0.010117643350920972, 0.00986242350930204, 0.002019274594219316, 0.0020033096439637955, 0.001839485484869948, 0.0011070667479019195, 0.0007509347764738704, 0.0005929928347402267, 0.0005604593737351515, 0.0003651158385464411]\n",
      "\n",
      "Language: [0.01946840059257822, 0.005744003966979579, 0.0034020119436348915, 0.002763563597916695, 0.0026289003086992606, 0.001915187175861417, 0.0013518906324426674, 0.001329464256059351, 0.0011332129000156603, 0.0010491368737445823]\n",
      "\n",
      "Country: [0.011461036370303601, 0.005162203028941863, 0.0040302742548811435, 0.0028138080049298834, 0.0018831023599369815, 0.0015947106980775932, 0.0015032522813514275, 0.0013518906324426674, 0.001329464256059351, 0.0012880417825089823]\n"
     ]
    }
   ],
   "source": [
    "# evaluate the mutual information scores of each feature in the dataset and print the highest in sorted order\n",
    "\n",
    "mi_scores = mutual_info_classif(train.iloc[:,:-1], train.iloc[:,-1], discrete_features=False, n_neighbors=9)\n",
    "print(f\"Original continuous features: {sorted(mi_scores, reverse=True)[0:10]}\\n\")\n",
    "\n",
    "mi_scores = mutual_info_classif(train_actor_1_name, train.iloc[:,-1], discrete_features=True, n_neighbors=9)\n",
    "print(f\"Actor 1: {sorted(mi_scores, reverse=True)[0:10]}\\n\")\n",
    "\n",
    "mi_scores = mutual_info_classif(train_actor_2_name, train.iloc[:,-1], discrete_features=True, n_neighbors=9)\n",
    "print(f\"Actor 2: {sorted(mi_scores, reverse=True)[0:10]}\\n\")\n",
    "\n",
    "mi_scores = mutual_info_classif(train_director_name, train.iloc[:,-1], discrete_features=True, n_neighbors=9)\n",
    "print(f\"Director: {sorted(mi_scores, reverse=True)[0:10]}\\n\")\n",
    "\n",
    "mi_scores = mutual_info_classif(train_genres, train.iloc[:,-1], discrete_features=False, n_neighbors=9)\n",
    "print(f\"Genres: {sorted(mi_scores, reverse=True)[0:10]}\\n\")\n",
    "\n",
    "mi_scores = mutual_info_classif(train_plot_keywords, train.iloc[:,-1], discrete_features=False, n_neighbors=9)\n",
    "print(f\"Plot keywords: {sorted(mi_scores, reverse=True)[0:10]}\\n\")\n",
    "\n",
    "mi_scores = mutual_info_classif(train_title_embedding, train.iloc[:,-1], discrete_features=False, n_neighbors=9)\n",
    "print(f\"Title embeddings: {sorted(mi_scores, reverse=True)[0:10]}\\n\")\n",
    "\n",
    "mi_scores = mutual_info_classif(train_content_rating, train.iloc[:,-1], discrete_features=True, n_neighbors=9)\n",
    "print(f\"Content rating: {sorted(mi_scores, reverse=True)[0:10]}\\n\")\n",
    "\n",
    "mi_scores = mutual_info_classif(train_language, train.iloc[:,-1], discrete_features=True, n_neighbors=9)\n",
    "print(f\"Language: {sorted(mi_scores, reverse=True)[0:10]}\\n\")\n",
    "\n",
    "mi_scores = mutual_info_classif(train_country, train.iloc[:,-1], discrete_features=True, n_neighbors=9)\n",
    "print(f\"Country: {sorted(mi_scores, reverse=True)[0:10]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590e9ac5",
   "metadata": {},
   "source": [
    "## Feature Combinations of High-MI Features using Polynomial Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "739557dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest MI continuous features: ['num_critic_for_reviews' 'duration' 'director_facebook_likes'\n",
      " 'num_voted_users' 'num_user_for_reviews' 'movie_facebook_likes']\n",
      "\n",
      "Original continuous feature combinations: [0.17667988759507036, 0.17455997823467895, 0.17288838232017945, 0.1629240446020579, 0.16235615896105182, 0.15996964717325834, 0.1565325868557288, 0.15451985892431352, 0.1531338214089062, 0.15284811836171164, 0.15152185023508258, 0.15023885207372967, 0.14993124058245577, 0.14898916976860432, 0.1488764264169964, 0.14863916530124088, 0.14689367742615245, 0.1466123009535938, 0.14287772653195896, 0.1424995773872202]\n",
      "\n",
      "Highest MI genre features: ['x18' 'x31' 'x32' 'x40' 'x85' 'x89']\n",
      "\n",
      "Genre feature combinations: [0.05037355885700068, 0.048849410097768775, 0.04814406698732654, 0.04808201970921244, 0.04800306123349252, 0.047558777411201625, 0.04753448458271725, 0.0469442188376652, 0.04661407831241737, 0.046389028642880525]\n",
      "\n",
      "Highest MI title embedding features: ['x15' 'x20' 'x24' 'x44' 'x50' 'x63']\n",
      "\n",
      "Title embedding feature combinations: [0.022144793281389852, 0.020051599705889434, 0.01992201607077071, 0.019845245641602727, 0.019488904264992613]\n"
     ]
    }
   ],
   "source": [
    "# SOME OF THE CODE BELOW HAS BEEN ADAPTED FROM ANSWERS POSTED ON STACKOVERFLOW, LINK IS BELOW:\n",
    "# https://stackoverflow.com/questions/39839112/the-easiest-way-for-getting-feature-names-after-running-selectkbest-in-scikit-le\n",
    "\n",
    "# select the 6-highest MI features and create degree-6 polynomial features using these\n",
    "\n",
    "mi = SelectKBest(mutual_info_classif, k=6)\n",
    "poly = PolynomialFeatures(degree=6)\n",
    "\n",
    "\n",
    "# ORIGINAL CONTINUOUS FEATURES\n",
    "# get the highest MI features\n",
    "mi.fit(train.iloc[:,:-1], train.iloc[:,-1])\n",
    "print(f\"Highest MI continuous features: {mi.get_feature_names_out()}\\n\")\n",
    "col_index = mi.get_support(indices=True)\n",
    "train_poly = pd.DataFrame(train.iloc[:,:-1]).iloc[:,col_index]\n",
    "test_poly = pd.DataFrame(test).iloc[:,col_index]\n",
    "\n",
    "# create the polynomial features\n",
    "train_poly = poly.fit_transform(train_poly)\n",
    "test_poly = poly.transform(test_poly)\n",
    "col_names = poly.get_feature_names_out()\n",
    "\n",
    "# standardise the features\n",
    "train_poly = std.fit_transform(train_poly)\n",
    "test_poly = std.transform(test_poly)\n",
    "\n",
    "# create a df for these features and calculate the mi scores for them\n",
    "train_poly = pd.DataFrame(train_poly, columns=col_names)\n",
    "test_poly = pd.DataFrame(test_poly, columns=col_names)\n",
    "\n",
    "mi_scores = mutual_info_classif(train_poly, train.iloc[:,-1], discrete_features=False, n_neighbors=9)\n",
    "print(f\"Original continuous feature combinations: {sorted(mi_scores, reverse=True)[0:20]}\\n\")\n",
    "\n",
    "# ABOVE PROCESS IS REPEATED FOR THE BELOW TWO DATA COLLECTIONS\n",
    "# GENRES\n",
    "mi.fit(train_genres, train.iloc[:,-1])\n",
    "print(f\"Highest MI genre features: {mi.get_feature_names_out()}\\n\")\n",
    "col_index = mi.get_support(indices=True)\n",
    "train_genre_poly = pd.DataFrame(train_genres).iloc[:,col_index]\n",
    "test_genre_poly = pd.DataFrame(test_genres).iloc[:,col_index]\n",
    "\n",
    "train_genre_poly = poly.fit_transform(train_genre_poly)\n",
    "test_genre_poly = poly.transform(test_genre_poly)\n",
    "col_names = poly.get_feature_names_out()\n",
    "\n",
    "train_genre_poly = std.fit_transform(train_genre_poly)\n",
    "test_genre_poly = std.transform(test_genre_poly)\n",
    "\n",
    "train_genre_poly = pd.DataFrame(train_genre_poly, columns=col_names)\n",
    "test_genre_poly = pd.DataFrame(test_genre_poly, columns=col_names)\n",
    "\n",
    "mi_scores = mutual_info_classif(train_genre_poly, train.iloc[:,-1], discrete_features=False, n_neighbors=9)\n",
    "print(f\"Genre feature combinations: {sorted(mi_scores, reverse=True)[0:10]}\\n\")\n",
    "\n",
    "\n",
    "# TITLE EMBEDDINGS\n",
    "mi.fit(train_title_embedding, train.iloc[:,-1])\n",
    "print(f\"Highest MI title embedding features: {mi.get_feature_names_out()}\\n\")\n",
    "col_index = mi.get_support(indices=True)\n",
    "train_title_poly = pd.DataFrame(train_title_embedding).iloc[:,col_index]\n",
    "test_title_poly = pd.DataFrame(test_title_embedding).iloc[:,col_index]\n",
    "\n",
    "train_title_poly = poly.fit_transform(train_title_poly)\n",
    "test_title_poly = poly.transform(test_title_poly)\n",
    "col_names = poly.get_feature_names_out()\n",
    "\n",
    "train_title_poly = std.fit_transform(train_title_poly)\n",
    "test_title_poly = std.transform(test_title_poly)\n",
    "\n",
    "train_title_poly = pd.DataFrame(train_title_poly, columns=col_names)\n",
    "test_title_poly = pd.DataFrame(test_title_poly, columns=col_names)\n",
    "\n",
    "mi_scores = mutual_info_classif(train_title_poly, train.iloc[:,-1], discrete_features=False, n_neighbors=9)\n",
    "print(f\"Title embedding feature combinations: {sorted(mi_scores, reverse=True)[0:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "138dfdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous features selected: ['num_voted_users' 'num_voted_users^2'\n",
      " 'director_facebook_likes^2 num_voted_users' 'num_voted_users^3'\n",
      " 'duration director_facebook_likes num_voted_users^2'\n",
      " 'director_facebook_likes^3 num_voted_users'\n",
      " 'director_facebook_likes num_voted_users^3' 'num_voted_users^4'\n",
      " 'num_critic_for_reviews^2 num_voted_users^3'\n",
      " 'duration^2 num_voted_users^3'\n",
      " 'duration^2 num_voted_users movie_facebook_likes^2'\n",
      " 'duration director_facebook_likes num_voted_users^2 movie_facebook_likes'\n",
      " 'duration num_voted_users^4' 'director_facebook_likes^4 num_voted_users'\n",
      " 'director_facebook_likes^2 num_voted_users^3' 'num_voted_users^5'\n",
      " 'num_voted_users^4 movie_facebook_likes' 'duration num_voted_users^5'\n",
      " 'director_facebook_likes num_voted_users^4 movie_facebook_likes'\n",
      " 'num_voted_users^6']\n",
      "\n",
      "Genre features selected: ['x0^2 x2 x3' 'x3^3 x5' 'x1 x3^4' 'x4 x5^4' 'x0^3 x2^2 x4'\n",
      " 'x0^2 x1 x2 x4 x5' 'x0^2 x2 x4^2 x5' 'x0 x3 x4 x5^3' 'x1 x3^3 x4 x5'\n",
      " 'x3^4 x4 x5']\n",
      "\n",
      "Title embedding features selected: ['x0 x4^2' 'x4 x5^2' 'x1^5' 'x1 x4 x5^3' 'x0^2 x1 x2^2 x3']\n"
     ]
    }
   ],
   "source": [
    "# find and select the 20 best original continuous polynomial features\n",
    "mi_poly = SelectKBest(mutual_info_classif, k=20)\n",
    "mi_poly.fit(train_poly, train.iloc[:,-1])\n",
    "col_index = mi_poly.get_support(indices=True)\n",
    "train_poly_best = pd.DataFrame(train_poly).iloc[:,col_index]\n",
    "test_poly_best = pd.DataFrame(test_poly).iloc[:,col_index]\n",
    "print(f\"Continuous features selected: {mi_poly.get_feature_names_out()}\\n\")\n",
    "\n",
    "# find and select the 10 best genre polynomial features\n",
    "mi_poly = SelectKBest(mutual_info_classif, k=10)\n",
    "mi_poly.fit(train_genre_poly, train.iloc[:,-1])\n",
    "col_index = mi_poly.get_support(indices=True)\n",
    "train_genre_best = pd.DataFrame(train_genre_poly).iloc[:,col_index]\n",
    "test_genre_best = pd.DataFrame(test_genre_poly).iloc[:,col_index]\n",
    "print(f\"Genre features selected: {mi_poly.get_feature_names_out()}\\n\")\n",
    "\n",
    "# find and select the 5 best title embedding polynomial features\n",
    "mi_poly = SelectKBest(mutual_info_classif, k=5)\n",
    "mi_poly.fit(train_title_poly, train.iloc[:,-1])\n",
    "col_index = mi_poly.get_support(indices=True)\n",
    "train_title_best = pd.DataFrame(train_title_poly).iloc[:,col_index]\n",
    "test_title_best = pd.DataFrame(test_title_poly).iloc[:,col_index]\n",
    "print(f\"Title embedding features selected: {mi_poly.get_feature_names_out()}\")\n",
    "\n",
    "# combine the features into a dataframe\n",
    "train_overall = pd.concat([train_poly_best, train_genre_best, train_title_best], axis=1)\n",
    "test_overall = pd.concat([test_poly_best, test_genre_best, test_title_best], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d0bbb1",
   "metadata": {},
   "source": [
    "# Classifiers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350fe61a",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57941da0",
   "metadata": {},
   "source": [
    "## AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99253a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 estimators and 1.0 learning rate: 0.590875968992248\n",
      "25 estimators and 1.25 learning rate: 0.4873565891472868\n",
      "25 estimators and 1.5 learning rate: 0.4344141749723145\n",
      "25 estimators and 1.75 learning rate: 0.32825359911406427\n",
      "25 estimators and 2.0 learning rate: 0.5995980066445182\n",
      "25 estimators and 2.25 learning rate: 0.49265227021040975\n",
      "50 estimators and 1.0 learning rate: 0.506\n",
      "50 estimators and 1.25 learning rate: 0.4364595791805094\n",
      "50 estimators and 1.5 learning rate: 0.36286932447397563\n",
      "50 estimators and 1.75 learning rate: 0.3269091915836102\n",
      "50 estimators and 2.0 learning rate: 0.13111627906976744\n",
      "50 estimators and 2.25 learning rate: 0.15576190476190477\n",
      "75 estimators and 1.0 learning rate: 0.4663798449612403\n",
      "75 estimators and 1.25 learning rate: 0.381516057585825\n",
      "75 estimators and 1.5 learning rate: 0.33156921373200443\n",
      "75 estimators and 1.75 learning rate: 0.31621483942414175\n",
      "75 estimators and 2.0 learning rate: 0.572922480620155\n",
      "75 estimators and 2.25 learning rate: 0.14407308970099666\n",
      "100 estimators and 1.0 learning rate: 0.4314407530454042\n",
      "100 estimators and 1.25 learning rate: 0.34023034330011076\n",
      "100 estimators and 1.5 learning rate: 0.3208781838316722\n",
      "100 estimators and 1.75 learning rate: 0.31157142857142855\n",
      "100 estimators and 2.0 learning rate: 0.14974418604651163\n",
      "100 estimators and 2.25 learning rate: 0.1022015503875969\n"
     ]
    }
   ],
   "source": [
    "# Tune the learning rate and number of base estimators used for ABC\n",
    "\n",
    "for i in [25, 50, 75, 100]:\n",
    "    for j in [1.0, 1.25, 1.5, 1.75, 2.0, 2.25]:\n",
    "        abc = AdaBoostClassifier(n_estimators=i, learning_rate=j)\n",
    "        abc.fit(train_overall, train.iloc[:,-1])\n",
    "        abc_acc = cross_val_score(abc, train_overall, train.iloc[:,-1], cv = 10)\n",
    "        print(f\"{i} estimators and {j} learning rate: {mean(abc_acc)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f14e9c",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40721a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 estimators: 0.6810874861572536\n",
      "50 estimators: 0.6854163898117386\n",
      "75 estimators: 0.6934119601328903\n",
      "100 estimators: 0.6880841638981174\n",
      "125 estimators: 0.6914119601328904\n",
      "150 estimators: 0.6894119601328904\n"
     ]
    }
   ],
   "source": [
    "# Tune the number of trees in the forest for RFC\n",
    "\n",
    "for i in [25, 50, 75, 100, 125, 150]:\n",
    "    rfc = RandomForestClassifier(n_estimators=i)\n",
    "    rfc.fit(train_overall, train.iloc[:,-1])\n",
    "    rfc_acc = cross_val_score(rfc, train_overall, train.iloc[:,-1], cv = 10)\n",
    "    print(f\"{i} estimators: {mean(rfc_acc)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d6655d",
   "metadata": {},
   "source": [
    "## Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66bb1139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier() base classifier and 10 estimators: 0.6880808416389812\n",
      "DecisionTreeClassifier() base classifier and 20 estimators: 0.6900797342192692\n",
      "DecisionTreeClassifier() base classifier and 30 estimators: 0.692407530454042\n",
      "DecisionTreeClassifier() base classifier and 40 estimators: 0.6924152823920265\n",
      "DecisionTreeClassifier() base classifier and 50 estimators: 0.691750830564784\n",
      "SVC() base classifier and 10 estimators: 0.6910874861572536\n",
      "SVC() base classifier and 20 estimators: 0.6934119601328904\n",
      "SVC() base classifier and 30 estimators: 0.694079734219269\n",
      "SVC() base classifier and 40 estimators: 0.6954119601328903\n",
      "SVC() base classifier and 50 estimators: 0.6947452934662237\n",
      "GaussianNB() base classifier and 10 estimators: 0.695078626799557\n",
      "GaussianNB() base classifier and 20 estimators: 0.69174861572536\n",
      "GaussianNB() base classifier and 30 estimators: 0.6927541528239203\n",
      "GaussianNB() base classifier and 40 estimators: 0.6934174972314507\n",
      "GaussianNB() base classifier and 50 estimators: 0.6920730897009967\n"
     ]
    }
   ],
   "source": [
    "# Tune the base classifier and number of base estimators used for BC\n",
    "\n",
    "for classifier in [DecisionTreeClassifier(), SVC(), GaussianNB()]:\n",
    "    for i in [10, 20, 30, 40 , 50]:\n",
    "        bc = BaggingClassifier(estimator=classifier, n_estimators=i)\n",
    "        bc.fit(train_overall, train.iloc[:,-1])\n",
    "        bc_acc = cross_val_score(rfc, train_overall, train.iloc[:,-1], cv = 10)\n",
    "        print(f\"{classifier} base classifier and {i} estimators: {mean(bc_acc)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febc42d5",
   "metadata": {},
   "source": [
    "# Final Models:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439756e3",
   "metadata": {},
   "source": [
    "## Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f0ef3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ABC\n",
    "abc = AdaBoostClassifier(n_estimators=25, learning_rate=2.0)\n",
    "\n",
    "# RFC\n",
    "rfc = RandomForestClassifier(n_estimators=75)\n",
    "\n",
    "# BC\n",
    "bc = BaggingClassifier(estimator=SVC(), n_estimators=40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b32b537",
   "metadata": {},
   "source": [
    "## Confusion Matrix using Holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "113708a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a train-test split with a 20% holdout\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_overall, train.iloc[:,-1], test_size=0.2)\n",
    "\n",
    "for classifier in [(abc, \"AdaBoostClassifier\"), (rfc, \"RandomForestClassifier\"), (bc, \"BaggingClassifier\")]:\n",
    "    # train each model on the 80% data\n",
    "    classifier[0].fit(X_train, y_train)\n",
    "    y_pred = classifier[0].predict(X_test)\n",
    "    \n",
    "    # create a confusion matrix for the predictions\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.title(f\"Confusion Matrix for {classifier[1]}\")\n",
    "    plt.savefig(f\"confusionMatrix {classifier[1]}.png\")\n",
    "    plt.close(\"all\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b2493a",
   "metadata": {},
   "source": [
    "## Final Predictions Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "117e4da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for classifier in [(abc, \"AdaBoostClassifier\"), (rfc, \"RandomForestClassifier\"), (bc, \"BaggingClassifier\")]:\n",
    "    # train the model on the entire training set and output the predictions to a csv file\n",
    "    classifier[0].fit(train_overall, train.iloc[:,-1])\n",
    "    test_predictions['imdb_score_binned'] = classifier[0].predict(test_overall)\n",
    "    test_predictions.to_csv(f'test_predictions_{classifier[1]}.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36b1ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
